#!/usr/bin/env python
# encoding: utf-8
"""
relations.py - find questions related to each other.

Created by Maximillian Dornseif on 2010-01-16.
Copyright (c) 2010 Maximillian Dornseif. Consider it BSD Licensed
"""

import enums
import re
from relations_stoppwords import stoppwords

try:
    import editdist
except ImportError:
    editdist = None


# Feel free play with this values
# How many keywords to consider per Question
KEYWORD_COUNT = 9
# How to weight the Title / Question / Text
TITLE_WEIGTH = 1.5
# How to weight the Answer
BODY_WEIGTH = 1.0
# At which value we don't consider things similar enough to consider them?
SIMILARITY_CUTOFF = 0.4


_split_re = re.compile(r'\W+', re.UNICODE)
def tokenize_text(content):
    """Splits text in a list of lowercase words. i18n aware.

    Words longer than 31 characters or shorter than 2 characters are ignored."""

    for word in _split_re.split(content.lower()):
        if 2 < len(word) < 32:
            yield word


# adapted PageRank algorithm see http://www.cs.unt.edu/~rada/papers/mihalcea.emnlp04.pdf
# and the weighted version http://www.cs.unt.edu/~rada/papers/hassan.ieee07.pdf
# inspired by the PHP code in the Post-Plugin Library by RobMarsh - http://rmarsh.com/plugins/
# The code form RobMarsh is GPL licensed, but I consider this code an independent implementation
# of the papers above, so this code is not a 'derivate work' in the GPL sense.
def calculate_textrank(content, num_terms=20):
    """Returns a list of keywords for 'content' with a maximum of 'num_terms'.

    Content should be a list of tokens (words) generated by tokenize_text() or a similar function."""

    # build a directed graph with words as vertices and, as edges, the words which precede them
    prev_word = 'aaaaa' # fake text start
    graph = {}
    out_edges = {}
    for word in content:
        if len(word) < 3 or word in stoppwords:
            # ignore short words or stop words
            continue
        # count how often a certain word accures 'previously'
        graph.setdefault(word, {})[prev_word] = graph.get(word, {prev_word: 0}).get(prev_word, 0) + 1.0
        # update counter of previous word for following word
        out_edges[prev_word] = out_edges.get(prev_word, 0) + 1.0
        prev_word = word

    # initialise default rank
    oldrank = {}
    rank = {}
    for vertex, in_edges in graph.items():
        oldrank[vertex] = 0.25;

    n = len(graph)
    if len(graph):
        base = 0.15 / n
        error_margin = n * 0.005
        error = 1
        while error > error_margin:
            error = 0.0
            # edge-weighted PageRank calculation
            for vertex, in_edges in graph.items():
                r = 0.0
                for edge, weight in in_edges.items():
                    r += (weight * oldrank.get(edge, 0.0)) / out_edges[edge]
                rank[vertex] = base + 0.95 * r
                error += abs(rank[vertex] - oldrank[vertex])
            oldrank = rank;

    top_terms = sorted([(x[1], x[0]) for x in rank.items()], reverse=True)[:num_terms]
    return top_terms


def extract_keywords_textrank(content, num_terms=20):
    """Returns a list of keywords for 'content' with a maximum of 'num_terms'.

    Content should be a list of tokens (words) generated by tokenize_text() or a similar function."""
    return [x[1] for x in calculate_textrank(content, num_terms=20) if x[0] > 0.2]


def keywords_contained(keywords, content):
    """Calculate the fraction of keywords contained in text.

    >>> keywords_contained(['texas', 'weapons', 'test'], "Test Text aus Texas.")
    0.66666666666666663
    >>> keywords_contained(['texas', 'weapons', 'test'], "Bienen und Blumen.")
    0.0
    """
    hits = 0.0
    if not keywords:
        return 0.0
    for keyword in keywords:
        if keyword in content.lower():
            hits += 1
    return hits / len(keywords)


def similarity_unordered(text1, text2):
    """Calculates the similarity between two short strings.

    This is done by sorting the tokens in the string and then calculating the Levenstein Edit Distance
    via http://www.mindrot.org/projects/py-editdist/

    Returns a value between 0 and 1.

    >>> similarity_unordered('Ich bin müde', 'Ich bin müde')
    1.0
    >>> similarity_unordered('Ich bin müde', 'müde bin ich')
    1.0
    >>> similarity_unordered('Ich bin müde', 'Ich bin rüde')
    0.92307692307692313
    >>> similarity_unordered('Ich bin müde', 'Ich bin prüde')
    0.85714285714285721
    >>> similarity_unordered('Ich bin müde', 'Wenn dein starker Arm es will, stehen alle Räder still.')
    0.10909090909090913
    """

    if not editdist:
        # Module not installed, default to 1
        return 1.0
    tokens1 = ' '.join(sorted(tokenize_text(text1)))
    tokens2 = ' '.join(sorted(tokenize_text(text2)))
    maxlen = float(max([len(tokens1), len(tokens2)]))
    distance = editdist.distance(tokens1.encode('utf-8'), tokens2.encode('utf-8'))
    divisor = max([distance, maxlen])
    if not divisor:
        print repr(text1), repr(text1), maxlen, distance
        print repr(tokens1), repr(tokens2), maxlen, distance
        return 0
    return 1 - (distance / divisor)


def find_related_questions_helper(question, cutoff=0.4):
    """Find the closest related FAQ entries."""

    # Extract Keywords from Question and Answer
    keywords = extract_keywords_textrank(' '.join([question.text, question.answer]), KEYWORD_COUNT)
    similar = []
    # iterate over ALL other Questions (slow!)
    candidates = question.__class__.objects.exclude(id=question.id, protected=True)
    candidates = candidates.filter(status__exact=enums.STATUS_ACTIVE)
    for otherquestion in candidates:
        otherkeywords = extract_keywords_textrank(' '.join([question.text, otherquestion.answer]), KEYWORD_COUNT)
        # check if the keywords appear in others body
        keywords_in_body = keywords_contained(keywords, ' '.join([otherquestion.text, otherquestion.answer]))
        # check if the keywords from both bodies match (Levenstein & direct match)
        keywords_to_keywords1 = similarity_unordered(' '.join(keywords), ' '.join(otherkeywords))
        keywords_to_keywords2 = len(set(keywords).intersection(set(otherkeywords))) / KEYWORD_COUNT
        # check how mutch the titles match
        title_to_tile = similarity_unordered(question.text, otherquestion.text)
        score = (keywords_in_body * BODY_WEIGTH) \
                + keywords_to_keywords1 \
                + keywords_to_keywords2 \
                + (title_to_tile * TITLE_WEIGTH)
        similar.append((score,
                       (keywords_in_body, keywords_to_keywords1, keywords_to_keywords2, title_to_tile),
                       otherquestion))
    similar.sort(reverse=True)
    return [(score, scoredetails, questionobj) for (score, scoredetails, questionobj) in similar if score > cutoff]


def find_related_questions(question, num_questions=5, cutoff=None):
    """Returns a list of related Question objects.

    To do the calculation the question submitted is compared to ALL OTHER QUESTIONS IN THE SYSTEM.
    So this gets slow if you have lots of Questions in your Database.

    num_questions tells how many questions you want returned.
    cutoff defines the minimal similarity between questions you are willing to accept."""

    if not cutoff:
        cutoff = SIMILARITY_CUTOFF
    similar = find_related_questions_helper(question, cutoff)
    return [questionobj for (score, scoredetauks, questionobj) in similar[:num_questions] if score > cutoff]


if __name__ == '__main__':
    print extract_keywords_textrank(tokenize_text('''You can now optionally designate Group Headers allowing
    questions regarding similar sub-topics to be grouped together. A new view called faq_list_by_group
    and an associated template has been added in the example app that demonstrates how to take advantage
    of this capability. (The original templates are unchanged by this new functionality.)
    You can now designate questions and/or group headers as protected. Protected questions are only made
    visible to authenticated users.rn</p>rn<p>rnAdditionally the display of answers now includes an
    autoescape designation allowing you to embed HTML tags as you desire.'''))
